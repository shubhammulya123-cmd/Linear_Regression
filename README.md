# Linear Regression from Scratch (Gradient Descent) ğŸ“ˆ

This project implements **Simple Linear Regression** using **Gradient Descent** from scratch in Python and visualizes the best-fit line using `matplotlib`.

The dataset contains two columns:

- `Hours` â†’ input feature (independent variable)
- `Scores` â†’ target value (dependent variable)

---

## ğŸš€ Features

- Loads dataset using `pandas`
- Implements gradient descent manually (no sklearn)
- Trains a linear regression model:
  \[
  y = mx + b
  \]
- Plots:
  - Scatter plot of training data
  - Best-fit regression line

---

## ğŸ“‚ Project Structure

Linear_Regression/
â”‚
â”œâ”€â”€ archive/
â”‚ â””â”€â”€ score_updated.csv
â”‚
â”œâ”€â”€ linear_regression.py
â””â”€â”€ README.md


---

## ğŸ§  Gradient Descent Formula

The model is:

\[
y = mx + b
\]

We update parameters using gradient descent:

\[
m = m - L \cdot \frac{\partial}{\partial m}
\]
\[
b = b - L \cdot \frac{\partial}{\partial b}
\]

Where:

- `m` = slope
- `b` = intercept
- `L` = learning rate
- `epochs` = number of iterations

---

## âš™ï¸ Requirements

Install dependencies:

```bash
pip install pandas matplotlib


â–¶ï¸ How to Run

Run the Python file:

python linear_regression.py


ğŸ“Œ Notes on Learning Rate

The learning rate controls how fast the model learns.

Example:

L = 0.0001 â†’ very small steps â†’ needs more epochs to converge

L = 0.01 â†’ larger steps â†’ converges faster in fewer epochs

So:

small L + large epochs = slow but stable

large L + small epochs = fast learning (but can diverge if too large)

ğŸ“Š Output

The program generates a plot showing:

Black dots â†’ dataset points

Red line â†’ regression best-fit line learned using gradient descent
